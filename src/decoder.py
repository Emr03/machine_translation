import torch
import numpy as np
from .sublayers import *

class DecoderLayer(torch.nn.Module):

    def __init__(self, params):

        super(DecoderLayer, self).__init__()
        self.d_model = params["d_model"]
        self.masked_attn = SelfAttention(params)
        self.attn = SelfAttention(params)
        self.ffnn = FFNN(params)
        self.layer_norm_1 = torch.nn.LayerNorm(normalized_shape=self.d_model)
        self.layer_norm_2 = torch.nn.LayerNorm(normalized_shape=self.d_model)
        self.layer_norm_3 = torch.nn.LayerNorm(normalized_shape=self.d_model)
        self.dropout = torch.nn.Dropout(params["dropout"])

    def forward(self, dec_outputs, enc_outputs, mask):

        out = self.layer_norm_1(self.masked_attn(x_q=dec_outputs,
                                                 x_k=dec_outputs,
                                                 x_v=dec_outputs,
                                                 mask=mask) + dec_outputs)
        out = self.dropout(out)
        out = self.layer_norm_2(self.attn(x_q=out,
                                          x_k = enc_outputs,
                                          x_v = enc_outputs) + out)

        out = self.dropout(out)
        out = self.layer_norm_3(self.ffnn(out) + out)
        out = self.dropout(out)

        return out

class StackedDecoder(torch.nn.Module):

    def __init__(self, n_layers, params):

        super(StackedDecoder, self).__init__()
        self.d_model = params["d_model"]
        self.vocab_size = params["vocab_size"]
        self.pos_enc = PositionalEncoding(params)
        self.embedding_layer = torch.nn.Embedding(self.vocab_size, self.d_model)
        self.decoder_layers = [DecoderLayer(params) for _ in range(n_layers)]

    def forward(self, dec_outputs, enc_outputs, mask):
        """

        :param dec_outputs: in case of inference: words generated so far
                            in case of training: target sentence
        :param enc_outputs: latent vectors generated by encoder
        :param mask:
        :return:
        """
        dec_outputs = self.pos_enc(dec_outputs)
        for layer in self.decoder_layers:
            dec_outputs = layer(dec_outputs=dec_outputs, enc_outputs=enc_outputs, mask=mask)

        return dec_outputs

if __name__ == "__main__":
    from src.config import params
    # test decoder layer
    x = torch.zeros(20, 5, 512, dtype=torch.float32)
    m = torch.zeros(1, 5)
    m[:, 0] = 1
    dec_layer = DecoderLayer(params)
    out = dec_layer(dec_outputs=x, enc_outputs=x, mask=m)
    print(out.shape)

    # test decoder stack
    dec = StackedDecoder(n_layers=6, params=params)
    out = dec(x, x, m)
    print(out.shape)
